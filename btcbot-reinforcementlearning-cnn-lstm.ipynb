{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-05T14:36:03.29827Z","iopub.execute_input":"2022-01-05T14:36:03.298655Z","iopub.status.idle":"2022-01-05T14:36:03.341051Z","shell.execute_reply.started":"2022-01-05T14:36:03.29855Z","shell.execute_reply":"2022-01-05T14:36:03.340303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gym_anytrading","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:36:03.342616Z","iopub.execute_input":"2022-01-05T14:36:03.343202Z","iopub.status.idle":"2022-01-05T14:36:12.533563Z","shell.execute_reply.started":"2022-01-05T14:36:03.34316Z","shell.execute_reply":"2022-01-05T14:36:12.532697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Credit: The code to the below is from Phil Tabor\nimport torch as T\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nclass DeepQNetwork(nn.Module):\n    \n    def __init__(self,lr,input_dims,fc1_dims,fc2_dims,n_actions,  n_conv):\n        super(DeepQNetwork, self).__init__()\n        self.input_dims = input_dims\n        self.fc1_dims = fc1_dims\n        self. n_conv =  n_conv \n        self.pad = 1\n        self.fc2_dims = fc2_dims\n        self.n_actions = n_actions\n                \n        self.firstSEQ = T.nn.Sequential(T.nn.Conv1d(in_channels=self.input_dims[0], out_channels=n_conv, kernel_size=5, padding = self.pad +1), \n                                        T.nn.ReLU(),\n                                        T.nn.Conv1d(in_channels=n_conv, out_channels=n_conv*2, kernel_size=3, padding = self.pad),\n                                    \n                                        T.nn.ReLU(),\n                                        \n                                        T.nn.Dropout(p=0.2))#T.nn.BatchNorm1d(n_conv*2))\n                                        \n        self.lstm = T.nn.LSTM(input_size= self.input_dims[-1],hidden_size=self.fc1_dims)\n        self.secondSEQ = T.nn.Sequential(\n                                        #T.nn.Linear(self.fc1_dims, self.fc1_dims ),\n                                        \n                                        T.nn.ReLU(),\n                                        T.nn.Dropout(p=0.2),\n                                        T.nn.Linear(self.fc1_dims , self.n_actions))\n                                         \n        \n\n        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n        self.loss = nn.MSELoss()\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        #self.device = T.device('cpu')\n        self.to(self.device)\n        \n        \n    def forward(self, state):\n        x =self.firstSEQ(state)\n        x,_ =self.lstm(state)\n        x = x[:, -1, :]\n        actions = self.secondSEQ(x)\n        \n        \n        \n        return actions\n        \n\n\nclass Agent():\n    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,max_mem_size=100000, eps_end=0.01, eps_dec=5e-4,  n_neurones = 128, n_conv = 128):\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.eps_min = eps_end\n        self.eps_dec = eps_dec\n        self.n_conv = n_conv\n        self.input_dims = input_dims\n        self.lr = lr\n        self.action_space = [i for i in range(n_actions)]\n        self.mem_size = max_mem_size\n        self.batch_size = batch_size\n        self.mem_cntr = 0\n        print('init dqn')\n        self.Q_eval = DeepQNetwork(self.lr, n_actions=n_actions, input_dims=input_dims,fc1_dims=n_neurones, fc2_dims=n_neurones, n_conv = self.n_conv ).float()\n        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n    def store_transition(self, state, action, reward, state_, done):\n        index = self.mem_cntr % self.mem_size\n        self.state_memory[index] = state\n        self.new_state_memory[index] = state_\n        self.action_memory[index] = action\n        self.reward_memory[index] = reward\n        self.terminal_memory[index] = done\n        self.mem_cntr += 1\n    def choose_action(self, observation):\n        if np.random.random() > self.epsilon:\n            state = T.tensor([observation]).to(self.Q_eval.device)\n            \n            #actions = self.Q_eval.forward(state.reshape(1, self.input_dims[0]).float())\n            actions = self.Q_eval.forward(state.reshape(1, self.input_dims[0],self.input_dims[-1]).float())              \n            action = T.argmax(actions).item()\n\n        else:\n            action = np.random.choice(self.action_space)\n        return action\n    def learn(self):\n        if self.mem_cntr < self.batch_size:\n            return\n        self.Q_eval.optimizer.zero_grad()\n        max_mem = min(self.mem_cntr, self.mem_size)\n        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n        batch_index = np.arange(self.batch_size, dtype=np.int32)\n        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n        action_batch = self.action_memory[batch]\n        q_eval = self.Q_eval.forward(state_batch.float())[batch_index, action_batch]\n        if self.gamma >0: \n            q_next = self.Q_eval.forward(new_state_batch.float())\n            q_next[terminal_batch] = 0.0\n        \n            q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n        else:\n            q_target = reward_batch \n        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n        loss.backward()\n        self.Q_eval.optimizer.step()\n        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:43:47.275731Z","iopub.execute_input":"2022-01-05T14:43:47.276108Z","iopub.status.idle":"2022-01-05T14:43:47.328632Z","shell.execute_reply.started":"2022-01-05T14:43:47.276075Z","shell.execute_reply":"2022-01-05T14:43:47.327045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"def preprocess_btc(X, D):\n    size = 100\n    Y = []\n    b = X[size - 1]\n    for i,x in enumerate(X[size:]):\n        #print(X[i:i+size+1][-1],x)\n        m = np.mean(X[i:i+size+1])\n        s = np.std(X[i:i+size+1])\n        Y.append([x, (x-m)/s, (x - b)/s])\n        b = x\n    return np.array(Y)\"\"\"\n        \n#np.mean(np.array([btc[1:], btc_diff]), axis=1)    \n","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:36:13.811605Z","iopub.execute_input":"2022-01-05T14:36:13.811886Z","iopub.status.idle":"2022-01-05T14:36:13.819206Z","shell.execute_reply.started":"2022-01-05T14:36:13.811849Z","shell.execute_reply":"2022-01-05T14:36:13.818554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport gym\nimport gym_anytrading\nimport matplotlib.pyplot as plt\n\n\ndef plot_learning_curve(x, scores, epsilons, filename=\"\", lines=None):\n    fig=plt.figure()\n    ax=fig.add_subplot(111, label=\"1\")\n    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n    ax.plot(x, epsilons, color=\"C0\")\n    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n    ax.tick_params(axis='x', colors=\"C0\")\n    ax.tick_params(axis='y', colors=\"C0\")\n    N = len(scores)\n    running_avg = np.empty(N)\n    for t in range(N):\n        running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n    ax2.scatter(x, running_avg, color=\"C1\")\n    ax2.axes.get_xaxis().set_visible(False)\n    ax2.yaxis.tick_right()\n    ax2.set_ylabel('Score', color=\"C1\")\n    ax2.yaxis.set_label_position('right')\n    ax2.tick_params(axis='y', colors=\"C1\")\n    if lines is not None:\n        for line in lines:\n            plt.axvline(x=line)\n    #plt.savefig(filename)\n    \n    \n\n\nurl = 'https://drive.google.com/file/d/1VNcpTlUzb9ATlXSzZBBhxCQ4oshSBX6F/view?usp=sharing'\nurl2='https://drive.google.com/uc?id=' + url.split('/')[-2]\ndf = pd.read_csv('/kaggle/input/bitcoin-price-usd/main.csv')\nprint(df.columns)\ndef normalize(x,m,s):\n    return (x-m)/s\n\nbtc = np.array(df['Close'])\nbtc_diff = np.diff(btc)\n#btc_diff_norm = (btc_diff - btc_diff.mean())/btc_diff.std()\n\n\ndata = []\nfor i, x in enumerate(btc[1:]):\n    data.append([x, btc_diff[i]])\ndata = np.array(data)\n\nlen(data)\nbtc_mean = data.mean(axis = 0)   \nbtc_std = data.std(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:36:13.820775Z","iopub.execute_input":"2022-01-05T14:36:13.821253Z","iopub.status.idle":"2022-01-05T14:36:16.014326Z","shell.execute_reply.started":"2022-01-05T14:36:13.8212Z","shell.execute_reply":"2022-01-05T14:36:16.013565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_len = 5\ntrain_len = 50\nlst_env = []\nfor i in range(int(len(data)/train_len) -1 ):\n    if i>0:        \n        env = gym.make('stocks-v0',df = df[1:], frame_bound=(seq_len + (i-1)*train_len ,i*train_len), window_size=seq_len)  \n        lst_env.append(env)\n        \nlst_env_emp = []\nfor i in range(int((len(data)-train_len)/20) -1 ):\n       \n        env = gym.make('stocks-v0',df = df[1:], frame_bound=(seq_len + i ,train_len + i), window_size=seq_len)  \n        lst_env_emp.append(env)","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:43:04.025142Z","iopub.execute_input":"2022-01-05T14:43:04.025432Z","iopub.status.idle":"2022-01-05T14:43:15.952316Z","shell.execute_reply.started":"2022-01-05T14:43:04.025402Z","shell.execute_reply":"2022-01-05T14:43:15.951501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nagent = Agent(gamma=0.3, epsilon=1., batch_size=100, n_actions=2,eps_end=0.01, input_dims=[1,seq_len], eps_dec=2e-3, lr=0.01, n_neurones = 64, n_conv = 32)\n\nlst_env_shuf = lst_env_emp\n\n\n\n\n\nfor j in range(10):\n    rand_j = int(np.random.random()*len(lst_env_emp) - 2)\n    env = lst_env_emp[(j+1)*100]\n    e = j\n    \n    \n    n_games = 100\n    rand_noise = 10\n    indice = 1\n    PF=[]\n    scores, eps_history,X, TOTAL = [], [], [], [] \n    scaler_std = np.array([env.observation_space.sample()[indice] for i in range(1000)]).std(axis = 0)\n    #print(scaler_std, 'scaler_std', scaler_std.shape)\n    scaler_mean = np.array([env.observation_space.sample()[indice] for i in range(1000)]).mean(axis = 0)\n    #print(scaler_mean, 'scaler_mean', scaler_mean.shape)\n    #scaler_std = btc_std\n    #scaler_mean = btc_mean\n    for i in range(n_games):\n        \n        score = 0\n        done = False\n        observation_ = env.reset()\n        observation_ = np.flip(observation_, 0)\n        observation = normalize(observation_, scaler_mean,scaler_mean).T[indice,:]\n        \n        count = 0\n        Act = []\n        while not done:\n            count+=1\n            #action = agent.choose_action(observation)\n            \n            action = env.action_space.sample()\n            observation_, reward, done, info = env.step(action)\n            observation_ = np.flip(observation_, 0)\n           \n            r = (action*observation_[0][1] - (1-action)*observation_[0][1])\n            #reward = r\n            Act.append(action)\n            \n            score += reward\n            agent.store_transition(observation, action, r/100 ,normalize(observation_, scaler_mean,scaler_mean).T[indice,:], done)\n            agent.learn()\n            observation = normalize(observation_, scaler_mean,scaler_mean).T[indice,:]\n            \n        \n        print(i, ' , score = ', score,  \" epsilon = \", agent.epsilon, \" mean action \", np.mean(Act), 'samplePrice = ', observation_[-1][0])\n        scores.append(score)\n        eps_history.append(agent.epsilon)\n        X.append(i)\n        \n    plot_learning_curve(X, scores, eps_history,\"file\")\n    plt.show()\n    plt.figure(figsize=(8,4))\n    plt.cla()\n    env.render_all()\n    plt.show()\n        \n   ","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:44:35.565652Z","iopub.execute_input":"2022-01-05T14:44:35.566285Z","iopub.status.idle":"2022-01-05T14:51:31.448803Z","shell.execute_reply.started":"2022-01-05T14:44:35.566224Z","shell.execute_reply":"2022-01-05T14:51:31.448115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n#agent = Agent(gamma=0.0, epsilon=1., batch_size=100, n_actions=2,eps_end=0.0, input_dims=[1,seq_len], eps_dec=5e-4, lr=0.00001, n_neurones = 1024, n_conv = 512)\n\n\nfees = 0.00\n\nscores, eps_history,X, TOTAL = [], [], [], [] \nfor j in range(10):\n    rand_j = int(np.random.random()*len(lst_env_emp) - 2)\n    env = lst_env_emp[j+60]\n    e = j\n   \n   \n    \n    \n    n_games = 1\n    rand_noise = 10\n    indice = 1\n    PF=[]\n    for i in range(n_games):\n        \n        score = 0\n        done = False\n        observation = normalize(env.reset(), btc_mean,btc_std).T[indice,:]\n        observation_ = np.flip(observation_, 0)\n        #observation =env.reset()\n        count = 0\n        Act = []\n        while not done:\n            count+=1\n            action = agent.choose_action(observation)\n            #action = env.action_space.sample()\n            observation_, reward, done, info = env.step(action)\n            observation_ = np.flip(observation_, 0)\n            #print(observation, observation_)\n            PF.append([observation_[0][0], action])\n            r = (action*observation_[0][1] - (1-action)*observation_[0][1])\n            reward = r\n            Act.append(action)\n            #print(r)\n            score += reward\n            #agent.store_transition(observation, action, reward ,normalize(observation_, btc_mean,btc_std).T[indice,:], done)\n            #agent.learn()\n            observation = normalize(observation_, btc_mean,btc_std).T[indice,:]\n        \n        scores.append(score)\n        eps_history.append(agent.epsilon)\n        X.append(e)\n        print(e, ' , score = ', score,  \" epsilon = \", agent.epsilon, \" mean action \", np.mean(Act), 'samplePrice = ', observation_[0][0])\n    plt.figure(figsize=(15,6))\n    plt.cla()\n    env.render_all()\n    plt.show()\n\n    def de_normalize(x, m, s):\n        return np.array(x*s + m)\n    total = score\n    \n    print('total = ', total)\n        \n    TOTAL.append([total, int(count_actions), total/count_actions])\n#plot_learning_curve(X, scores, eps_history, \"plot_learning_curve\")\n\nprint(np.mean(TOTAL,axis=0))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:36:31.93238Z","iopub.execute_input":"2022-01-05T14:36:31.933385Z","iopub.status.idle":"2022-01-05T14:36:32.263433Z","shell.execute_reply.started":"2022-01-05T14:36:31.933344Z","shell.execute_reply":"2022-01-05T14:36:32.26218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_learning_curve(X, scores, eps_history,\"dddd\")\nobservation_","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:36:32.264515Z","iopub.status.idle":"2022-01-05T14:36:32.265411Z","shell.execute_reply.started":"2022-01-05T14:36:32.265148Z","shell.execute_reply":"2022-01-05T14:36:32.265174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nif __name__ == '__main__':\n    \n    env = lst_env_emp[0]\n    n_games = 1\n    rand_noise = 10\n    indice = 1\n    for i in range(n_games):\n        \n        score = 0\n        done = False\n        observation = env.reset()\n        print(np.flip(observation,0))\n        print(btc[1:21])\n        \n        #observation =env.reset()\n        count = 0\n        Act = []\n        PF = []\n        while not done:\n            count+=1\n            action =0\n            \n            observation_, reward, done, info = env.step(action)\n            #print(observation, observation_)\n            print(np.flip(observation_,0))\n            break\n            r = (-action*observation_[0][1] + (1-action)*observation_[0][1])\n            PF.append([observation_[0][0], action])\n            reward = r/10\n            Act.append(action)\n            \n            score += reward\n            agent.store_transition(observation, action, reward ,normalize(observation_, btc_mean,btc_std).T[indice,:], done)\n            #agent.learn()\n            observation = normalize(observation_, btc_mean,btc_std).T[indice,:]\n        \n        scores.append(score)\n        eps_history.append(agent.epsilon)\n        X.append(e)\n        print(e, ' , score = ', score,  \" epsilon = \", agent.epsilon, \" mean action \", np.mean(Act))\n        \"\"\"plt.figure(figsize=(15,6))\n        plt.cla()\n        env.render_all()\n        plt.show()\n        \"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:36:32.266799Z","iopub.status.idle":"2022-01-05T14:36:32.267216Z","shell.execute_reply.started":"2022-01-05T14:36:32.266992Z","shell.execute_reply":"2022-01-05T14:36:32.267015Z"},"trusted":true},"execution_count":null,"outputs":[]}]}